{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cb1bc876-d3f6-4cf1-9f2a-decf314883cf",
   "metadata": {},
   "source": [
    "Problem statement: Imagine you are working as a data scientist at a home electronics company which manufactures state of the art smart televisions. You want to develop a cool feature in the smart-TV that can recognise five different gestures performed by the user which will help users control the TV without using a remote. The gestures are continuously monitored by the webcam mounted on the TV. Each gesture corresponds to a specific command:\n",
    "\n",
    "1. Thumbs up: Increase the volume\n",
    "2. Thumbs down: Decrease the volume\n",
    "3. Left swipe: 'Jump' backwards 10 seconds\n",
    "4. Right swipe: 'Jump' forward 10 seconds\n",
    "5. Stop: Pause the movie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "48cf36d9-8a65-4014-8be1-9ee050ba9809",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing libraries required for the case study and to plot charts\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "#Importing Regular Expressions for String manipulation\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a85ee18d-3273-4d47-9f56-2c5dbfa112a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(30)\n",
    "import random as rn\n",
    "rn.seed(30)\n",
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(30)\n",
    "#tf.random.set_seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dca44605-c78d-4238-9060-8c69fcd20a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7753b169-688c-4755-bc12-b7433065237e",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'skimage'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mimageio\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m imread\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m keras\n\u001b[1;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mskimage\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtransform\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m resize\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcv2\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mdatetime\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'skimage'"
     ]
    }
   ],
   "source": [
    "# Importing the necessary libraries\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "from imageio import imread\n",
    "from tensorflow import keras\n",
    "from skimage.transform import resize\n",
    "import cv2\n",
    "import datetime\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import abc\n",
    "from sys import getsizeof"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "93bd62d4-a3ba-4995-a118-3491f439b447",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting skimage\n",
      "  Downloading skimage-0.0.tar.gz (757 bytes)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'error'\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  python setup.py egg_info did not run successfully.\n",
      "  exit code: 1\n",
      "  \n",
      "  [3 lines of output]\n",
      "  \n",
      "  *** Please install the `scikit-image` package (instead of `skimage`) ***\n",
      "  \n",
      "  [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "error: metadata-generation-failed\n",
      "\n",
      "Encountered error while generating package metadata.\n",
      "\n",
      "See above for output.\n",
      "\n",
      "note: This is an issue with the package mentioned above, not pip.\n",
      "hint: See above for details.\n"
     ]
    }
   ],
   "source": [
    "pip install skimage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "894ab43c-8eaa-4fe4-af30-ee040a4bca2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeableNote: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Collecting scikit-image\n",
      "  Obtaining dependency information for scikit-image from https://files.pythonhosted.org/packages/08/c0/8085c5fd2f7f7514a0c5031b666171d5828ac5b3c9cf5d0ecd19688d5407/scikit_image-0.21.0-cp311-cp311-win_amd64.whl.metadata\n",
      "  Downloading scikit_image-0.21.0-cp311-cp311-win_amd64.whl.metadata (14 kB)\n",
      "Requirement already satisfied: numpy>=1.21.1 in c:\\users\\nisch\\appdata\\roaming\\python\\python311\\site-packages (from scikit-image) (1.24.2)\n",
      "Requirement already satisfied: scipy>=1.8 in c:\\users\\nisch\\appdata\\roaming\\python\\python311\\site-packages (from scikit-image) (1.10.0)\n",
      "Collecting networkx>=2.8 (from scikit-image)\n",
      "  Downloading networkx-3.1-py3-none-any.whl (2.1 MB)\n",
      "     ---------------------------------------- 0.0/2.1 MB ? eta -:--:--\n",
      "     ---- ----------------------------------- 0.3/2.1 MB 7.9 MB/s eta 0:00:01\n",
      "     -------------- ------------------------- 0.8/2.1 MB 9.6 MB/s eta 0:00:01\n",
      "     ---------------------- ----------------- 1.2/2.1 MB 9.4 MB/s eta 0:00:01\n",
      "     ------------------------------- -------- 1.6/2.1 MB 9.3 MB/s eta 0:00:01\n",
      "     ---------------------------------------  2.0/2.1 MB 9.9 MB/s eta 0:00:01\n",
      "     ---------------------------------------  2.1/2.1 MB 9.4 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 2.1/2.1 MB 7.8 MB/s eta 0:00:00\n",
      "Requirement already satisfied: pillow>=9.0.1 in c:\\users\\nisch\\appdata\\roaming\\python\\python311\\site-packages (from scikit-image) (9.4.0)\n",
      "Requirement already satisfied: imageio>=2.27 in c:\\program files\\python311\\lib\\site-packages (from scikit-image) (2.31.3)\n",
      "Collecting tifffile>=2022.8.12 (from scikit-image)\n",
      "  Obtaining dependency information for tifffile>=2022.8.12 from https://files.pythonhosted.org/packages/12/3e/89513f44a10c625121b7d5bc54390d7ac7f2c92a19755c052888febf9730/tifffile-2023.8.30-py3-none-any.whl.metadata\n",
      "  Downloading tifffile-2023.8.30-py3-none-any.whl.metadata (30 kB)\n",
      "Collecting PyWavelets>=1.1.1 (from scikit-image)\n",
      "  Using cached PyWavelets-1.4.1-cp311-cp311-win_amd64.whl (4.2 MB)\n",
      "Requirement already satisfied: packaging>=21 in c:\\users\\nisch\\appdata\\roaming\\python\\python311\\site-packages (from scikit-image) (23.0)\n",
      "Collecting lazy_loader>=0.2 (from scikit-image)\n",
      "  Obtaining dependency information for lazy_loader>=0.2 from https://files.pythonhosted.org/packages/a1/c3/65b3814e155836acacf720e5be3b5757130346670ac454fee29d3eda1381/lazy_loader-0.3-py3-none-any.whl.metadata\n",
      "  Downloading lazy_loader-0.3-py3-none-any.whl.metadata (4.3 kB)\n",
      "Downloading scikit_image-0.21.0-cp311-cp311-win_amd64.whl (22.8 MB)\n",
      "   ---------------------------------------- 0.0/22.8 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.6/22.8 MB 19.8 MB/s eta 0:00:02\n",
      "   - -------------------------------------- 1.1/22.8 MB 11.7 MB/s eta 0:00:02\n",
      "   -- ------------------------------------- 1.5/22.8 MB 10.8 MB/s eta 0:00:02\n",
      "   --- ------------------------------------ 1.8/22.8 MB 9.0 MB/s eta 0:00:03\n",
      "   --- ------------------------------------ 2.1/22.8 MB 8.5 MB/s eta 0:00:03\n",
      "   ---- ----------------------------------- 2.5/22.8 MB 8.3 MB/s eta 0:00:03\n",
      "   ---- ----------------------------------- 2.8/22.8 MB 7.7 MB/s eta 0:00:03\n",
      "   ----- ---------------------------------- 3.1/22.8 MB 7.5 MB/s eta 0:00:03\n",
      "   ------ --------------------------------- 3.5/22.8 MB 7.8 MB/s eta 0:00:03\n",
      "   ------ --------------------------------- 4.0/22.8 MB 7.9 MB/s eta 0:00:03\n",
      "   ------- -------------------------------- 4.4/22.8 MB 8.1 MB/s eta 0:00:03\n",
      "   -------- ------------------------------- 4.8/22.8 MB 8.3 MB/s eta 0:00:03\n",
      "   --------- ------------------------------ 5.2/22.8 MB 8.1 MB/s eta 0:00:03\n",
      "   --------- ------------------------------ 5.7/22.8 MB 8.4 MB/s eta 0:00:03\n",
      "   ---------- ----------------------------- 6.1/22.8 MB 8.5 MB/s eta 0:00:02\n",
      "   ----------- ---------------------------- 6.5/22.8 MB 8.5 MB/s eta 0:00:02\n",
      "   ------------ --------------------------- 7.0/22.8 MB 8.6 MB/s eta 0:00:02\n",
      "   ------------- -------------------------- 7.4/22.8 MB 8.4 MB/s eta 0:00:02\n",
      "   ------------- -------------------------- 7.8/22.8 MB 8.6 MB/s eta 0:00:02\n",
      "   -------------- ------------------------- 8.3/22.8 MB 8.6 MB/s eta 0:00:02\n",
      "   --------------- ------------------------ 8.7/22.8 MB 8.6 MB/s eta 0:00:02\n",
      "   ---------------- ----------------------- 9.1/22.8 MB 8.6 MB/s eta 0:00:02\n",
      "   ---------------- ----------------------- 9.6/22.8 MB 8.6 MB/s eta 0:00:02\n",
      "   ----------------- ---------------------- 10.0/22.8 MB 8.7 MB/s eta 0:00:02\n",
      "   ------------------ --------------------- 10.5/22.8 MB 8.7 MB/s eta 0:00:02\n",
      "   ------------------- -------------------- 10.9/22.8 MB 8.5 MB/s eta 0:00:02\n",
      "   ------------------- -------------------- 11.3/22.8 MB 8.5 MB/s eta 0:00:02\n",
      "   -------------------- ------------------- 11.8/22.8 MB 8.6 MB/s eta 0:00:02\n",
      "   --------------------- ------------------ 12.2/22.8 MB 8.8 MB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 12.7/22.8 MB 9.0 MB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 13.1/22.8 MB 9.1 MB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 13.6/22.8 MB 9.4 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 14.0/22.8 MB 9.4 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 14.4/22.8 MB 9.2 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 14.9/22.8 MB 9.4 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 15.3/22.8 MB 9.4 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 15.7/22.8 MB 9.4 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 16.1/22.8 MB 9.2 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 16.6/22.8 MB 9.4 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 17.0/22.8 MB 9.2 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 17.5/22.8 MB 9.2 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 17.8/22.8 MB 9.2 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 18.3/22.8 MB 9.2 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 18.7/22.8 MB 9.2 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 19.1/22.8 MB 9.2 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 19.6/22.8 MB 9.2 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 20.0/22.8 MB 9.2 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 20.3/22.8 MB 9.1 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 20.6/22.8 MB 9.0 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 21.0/22.8 MB 9.0 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 21.3/22.8 MB 9.1 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 21.8/22.8 MB 9.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------  22.2/22.8 MB 9.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------  22.7/22.8 MB 9.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------  22.8/22.8 MB 8.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  22.8/22.8 MB 8.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  22.8/22.8 MB 8.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  22.8/22.8 MB 8.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 22.8/22.8 MB 7.6 MB/s eta 0:00:00\n",
      "Downloading lazy_loader-0.3-py3-none-any.whl (9.1 kB)\n",
      "Downloading tifffile-2023.8.30-py3-none-any.whl (221 kB)\n",
      "   ---------------------------------------- 0.0/221.9 kB ? eta -:--:--\n",
      "   ---------------------------------------- 221.9/221.9 kB 6.8 MB/s eta 0:00:00\n",
      "Installing collected packages: tifffile, PyWavelets, networkx, lazy_loader, scikit-image\n",
      "Successfully installed PyWavelets-1.4.1 lazy_loader-0.3 networkx-3.1 scikit-image-0.21.0 tifffile-2023.8.30\n"
     ]
    }
   ],
   "source": [
    "pip install scikit-image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d835eb12-e72d-415c-9bb6-8195a9b9836a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the necessary libraries\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "from imageio import imread\n",
    "from tensorflow import keras\n",
    "from skimage.transform import resize\n",
    "import cv2\n",
    "import datetime\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import abc\n",
    "from sys import getsizeof"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "20117876-ae23-48a4-9a4b-ea2afc8e0979",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_doc = np.random.permutation(open('train.csv').readlines())\n",
    "val_doc = np.random.permutation(open('val.csv').readlines())\n",
    "batch_size = 51"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25a3e30e-90fa-42ec-bd19-66d128883e8b",
   "metadata": {},
   "source": [
    "## Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "862444eb-a8a0-4be9-be96-b2d5df88ec21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator(source_path, folder_list, batch_size):\n",
    "    print( 'Source path = ', source_path, '; batch size =', batch_size)\n",
    "    img_idx = [11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28]#create a list of image numbers you want to use for a particular video\n",
    "    while True:\n",
    "        t = np.random.permutation(folder_list)\n",
    "        num_batches = len(source_path)//batch_size # calculate the number of batches\n",
    "        for batch in range(num_batches): # we iterate over the number of batches\n",
    "            batch_data = np.zeros((batch_size,18,100,100,3)) # x is the number of images you use for each video, (y,z) is the final size of the input images and 3 is the number of channels RGB\n",
    "            batch_labels = np.zeros((batch_size,5)) # batch_labels is the one hot representation of the output\n",
    "            for folder in range(batch_size): # iterate over the batch_size\n",
    "                imgs = os.listdir(source_path+'/'+ t[folder + (batch*batch_size)].split(';')[0]) # read all the images in the folder\n",
    "                for idx,item in enumerate(img_idx): #  Iterate iver the frames/images of a folder to read them in\n",
    "                    image = imageio.imread(source_path+'/'+ t[folder + (batch*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n",
    "                    \n",
    "                    \n",
    "                    \n",
    "                    # Cropped image of above dimension \n",
    "                    # (It will not change orginal image) \n",
    "                    \n",
    "                    #image = image.crop((0, 0, 120, 120))\n",
    "                    image = image.resize(100, 100)\n",
    "                    \n",
    "                    \n",
    "                    #crop the images and resize them. Note that the images are of 2 different shape \n",
    "                    #and the conv3D will throw error if the inputs in a batch have different shapes\n",
    "                    \n",
    "                    batch_data[folder,idx,:,:,0] /= 255\n",
    "                    batch_data[folder,idx,:,:,1] /= 255 #normalise and feed in the image\n",
    "                    batch_data[folder,idx,:,:,2] /= 255 #normalise and feed in the image\n",
    "                    \n",
    "                batch_labels[folder, int(t[folder + (batch*batch_size)].strip().split(';')[2])] = 1\n",
    "            yield batch_data, batch_labels #you yield the batch_data and the batch_labels, remember what does yield do\n",
    "\n",
    "        \n",
    "        # write the code for the remaining data points which are left after full batches\n",
    "        if((len(source_path)%batch_size)//2==0):\n",
    "            batch_size = 2\n",
    "        else:\n",
    "            batch_size = 1\n",
    "        num_batches = len(source_path)%batch_size # calculate the number of batches\n",
    "        for batch in range(num_batches): # we iterate over the number of batches\n",
    "            batch_data = np.zeros((batch_size,18,100,100,3)) # x is the number of images you use for each video, (y,z) is the final size of the input images and 3 is the number of channels RGB\n",
    "            batch_labels = np.zeros((batch_size,5)) # batch_labels is the one hot representation of the output\n",
    "            for folder in range(batch_size): # iterate over the batch_size\n",
    "                imgs = os.listdir(source_path+'/'+ t[folder + (batch*batch_size)].split(';')[0]) # read all the images in the folder\n",
    "                for idx,item in enumerate(img_idx): #  Iterate iver the frames/images of a folder to read them in\n",
    "                    image = imageio.imread(source_path+'/'+ t[folder + (batch*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n",
    "                    \n",
    "                    \n",
    "                    \n",
    "                    # Cropped image of above dimension \n",
    "                    # (It will not change orginal image) \n",
    "                    \n",
    "                    #image = image.crop((0, 0, 120, 120))\n",
    "                    image = image.resize(100, 100)\n",
    "                    \n",
    "                    \n",
    "                    #crop the images and resize them. Note that the images are of 2 different shape \n",
    "                    #and the conv3D will throw error if the inputs in a batch have different shapes\n",
    "                    \n",
    "                    batch_data[folder,idx,:,:,0] /= 255\n",
    "                    batch_data[folder,idx,:,:,1] /= 255 #normalise and feed in the image\n",
    "                    batch_data[folder,idx,:,:,2] /= 255 #normalise and feed in the image\n",
    "                    \n",
    "                batch_labels[folder, int(t[folder + (batch*batch_size)].strip().split(';')[2])] = 1\n",
    "            yield batch_data, batch_labels #you yield the batch_data and the batch_labels, remember what does yield do"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ea8923b4-2711-45d0-8ed4-4ce8e0b8f985",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# training sequences = 663\n",
      "# validation sequences = 100\n",
      "# epochs = 15\n"
     ]
    }
   ],
   "source": [
    "curr_dt_time = datetime.datetime.now()\n",
    "train_path = 'train'\n",
    "val_path = 'val'\n",
    "num_train_sequences = len(train_doc)\n",
    "print('# training sequences =', num_train_sequences)\n",
    "num_val_sequences = len(val_doc)\n",
    "print('# validation sequences =', num_val_sequences)\n",
    "num_epochs = 15 # choose the number of epochs\n",
    "print ('# epochs =', num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "409b53f7-c379-44a9-8287-89b367e8ea7d",
   "metadata": {},
   "source": [
    "## Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a88b1cac-c65c-4fa0-a9c9-f0583684d93a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.layers import Dense, GRU, Flatten, TimeDistributed, Flatten, BatchNormalization, Activation, Dropout\n",
    "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
    "from keras import optimizers\n",
    "from tensorflow.keras.layers import Conv3D\n",
    "from tensorflow.keras.layers import MaxPooling3D\n",
    "\n",
    "\n",
    "\n",
    "#write your model here\n",
    "Input_shape = (18, 100, 100, 3)\n",
    "model = Sequential()\n",
    "model.add(Conv3D(32, (3,3,3), padding='same',\n",
    "                 input_shape=Input_shape))\n",
    "#model.add(Conv3D(32, kernel_size=(3, 3, 3), activation='relu', kernel_initializer='he_uniform', input_shape=sample_shape))\n",
    "model.add(Activation('relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv3D(32, (3, 3,3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling3D(pool_size=(2, 2,2)))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Conv3D(64, (3, 3,3), padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv3D(64, (3, 3,3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling3D(pool_size=(2, 2,2)))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(5))\n",
    "model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92f66edf-f8af-48e7-b6bc-b850a902f54c",
   "metadata": {},
   "source": [
    "Now that you have written the model, the next step is to compile the model. When you print the summary of the model, you'll see the total number of parameters you have to train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ae5f2a55-67ef-40d7-a5c6-2fb53ce98279",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "83e77190-b2e7-4a67-b011-fbdc601af68b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9b4ad14e-18b2-4dae-bc9a-58b16d223940",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv3d_2 (Conv3D)           (None, 18, 100, 100, 32   2624      \n",
      "                             )                                   \n",
      "                                                                 \n",
      " activation_2 (Activation)   (None, 18, 100, 100, 32   0         \n",
      "                             )                                   \n",
      "                                                                 \n",
      " batch_normalization_2 (Bat  (None, 18, 100, 100, 32   128       \n",
      " chNormalization)            )                                   \n",
      "                                                                 \n",
      " conv3d_3 (Conv3D)           (None, 16, 98, 98, 32)    27680     \n",
      "                                                                 \n",
      " activation_3 (Activation)   (None, 16, 98, 98, 32)    0         \n",
      "                                                                 \n",
      " batch_normalization_3 (Bat  (None, 16, 98, 98, 32)    128       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " max_pooling3d (MaxPooling3  (None, 8, 49, 49, 32)     0         \n",
      " D)                                                              \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 8, 49, 49, 32)     0         \n",
      "                                                                 \n",
      " conv3d_4 (Conv3D)           (None, 8, 49, 49, 64)     55360     \n",
      "                                                                 \n",
      " activation_4 (Activation)   (None, 8, 49, 49, 64)     0         \n",
      "                                                                 \n",
      " batch_normalization_4 (Bat  (None, 8, 49, 49, 64)     256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " conv3d_5 (Conv3D)           (None, 6, 47, 47, 64)     110656    \n",
      "                                                                 \n",
      " activation_5 (Activation)   (None, 6, 47, 47, 64)     0         \n",
      "                                                                 \n",
      " batch_normalization_5 (Bat  (None, 6, 47, 47, 64)     256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " max_pooling3d_1 (MaxPoolin  (None, 3, 23, 23, 64)     0         \n",
      " g3D)                                                            \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 3, 23, 23, 64)     0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 101568)            0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 512)               52003328  \n",
      "                                                                 \n",
      " activation_6 (Activation)   (None, 512)               0         \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 512)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 5)                 2565      \n",
      "                                                                 \n",
      " activation_7 (Activation)   (None, 5)                 0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 52202981 (199.14 MB)\n",
      "Trainable params: 52202597 (199.14 MB)\n",
      "Non-trainable params: 384 (1.50 KB)\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "optimiser = keras.optimizers.Adam(lr=0.001)\n",
    "model.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "print (model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "51ec9c33-5e44-441c-aaef-4f85b625075b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = generator(train_path, train_doc, batch_size)\n",
    "val_generator = generator(val_path, val_doc, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "61ff6059-9966-4965-92f4-a5ccdbcc034c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`epsilon` argument is deprecated and will be removed, use `min_delta` instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`epsilon` argument is deprecated and will be removed, use `min_delta` instead.\n"
     ]
    }
   ],
   "source": [
    "model_name = 'Gesture_recog' + '_' + str(curr_dt_time).replace(' ','').replace(':','_') + '/'\n",
    "    \n",
    "if not os.path.exists(model_name):\n",
    "    os.mkdir(model_name)\n",
    "        \n",
    "filepath = model_name + 'model-{epoch:05d}-{loss:.5f}-{categorical_accuracy:.5f}-{val_loss:.5f}-{val_categorical_accuracy:.5f}.h5'\n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=False, save_weights_only=False, mode='auto', period=1)\n",
    "\n",
    "LR = ReduceLROnPlateau(monitor = \"val_loss\", factor = 0.1, patience = 10,\n",
    "  verbose = 0, mode = \"auto\", epsilon = 1e-04, cooldown = 0,\n",
    "  min_lr = 0)\n",
    "callbacks_list = [checkpoint, LR]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5f78ee79-1d08-4987-a1d2-9dc73b1d459d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if (num_train_sequences%batch_size) == 0:\n",
    "    steps_per_epoch = int(num_train_sequences/batch_size)\n",
    "else:\n",
    "    steps_per_epoch = (num_train_sequences//batch_size) + 1\n",
    "\n",
    "if (num_val_sequences%batch_size) == 0:\n",
    "    validation_steps = int(num_val_sequences/batch_size)\n",
    "else:\n",
    "    validation_steps = (num_val_sequences//batch_size) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f774ab27-7ecf-47a0-85d7-e7c66faf1e1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source path =  train ; batch size = 51\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] The system cannot find the path specified: 'train/WIN_20180925_17_55_45_Pro_Thumbs_Down_new'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[39], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_generator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_generator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msteps_per_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msteps_per_epoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_generator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mvalidation_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_steps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mworkers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minitial_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\engine\\training.py:2810\u001b[0m, in \u001b[0;36mModel.fit_generator\u001b[1;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[0;32m   2798\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Fits the model on data yielded batch-by-batch by a Python generator.\u001b[39;00m\n\u001b[0;32m   2799\u001b[0m \n\u001b[0;32m   2800\u001b[0m \u001b[38;5;124;03mDEPRECATED:\u001b[39;00m\n\u001b[0;32m   2801\u001b[0m \u001b[38;5;124;03m  `Model.fit` now supports generators, so there is no longer any need to\u001b[39;00m\n\u001b[0;32m   2802\u001b[0m \u001b[38;5;124;03m  use this endpoint.\u001b[39;00m\n\u001b[0;32m   2803\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   2804\u001b[0m warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m   2805\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`Model.fit_generator` is deprecated and \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2806\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwill be removed in a future version. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2807\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease use `Model.fit`, which supports generators.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   2808\u001b[0m     stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[0;32m   2809\u001b[0m )\n\u001b[1;32m-> 2810\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2811\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgenerator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2812\u001b[0m \u001b[43m    \u001b[49m\u001b[43msteps_per_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msteps_per_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2813\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2814\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2815\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2816\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2817\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2818\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_freq\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_freq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2819\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclass_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2820\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_queue_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_queue_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2821\u001b[0m \u001b[43m    \u001b[49m\u001b[43mworkers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mworkers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2822\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_multiprocessing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_multiprocessing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2823\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshuffle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2824\u001b[0m \u001b[43m    \u001b[49m\u001b[43minitial_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minitial_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2825\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "Cell \u001b[1;32mIn[16], line 11\u001b[0m, in \u001b[0;36mgenerator\u001b[1;34m(source_path, folder_list, batch_size)\u001b[0m\n\u001b[0;32m      9\u001b[0m batch_labels \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros((batch_size,\u001b[38;5;241m5\u001b[39m)) \u001b[38;5;66;03m# batch_labels is the one hot representation of the output\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m folder \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(batch_size): \u001b[38;5;66;03m# iterate over the batch_size\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m     imgs \u001b[38;5;241m=\u001b[39m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource_path\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m[\u001b[49m\u001b[43mfolder\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m;\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# read all the images in the folder\u001b[39;00m\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m idx,item \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(img_idx): \u001b[38;5;66;03m#  Iterate iver the frames/images of a folder to read them in\u001b[39;00m\n\u001b[0;32m     13\u001b[0m         image \u001b[38;5;241m=\u001b[39m imageio\u001b[38;5;241m.\u001b[39mimread(source_path\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m+\u001b[39m t[folder \u001b[38;5;241m+\u001b[39m (batch\u001b[38;5;241m*\u001b[39mbatch_size)]\u001b[38;5;241m.\u001b[39mstrip()\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m;\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m+\u001b[39mimgs[item])\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mfloat32)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] The system cannot find the path specified: 'train/WIN_20180925_17_55_45_Pro_Thumbs_Down_new'"
     ]
    }
   ],
   "source": [
    "model.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n",
    "                    callbacks=callbacks_list, validation_data=val_generator, \n",
    "                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6db080b-fd98-40a7-b373-5318d67b7723",
   "metadata": {},
   "source": [
    "## Model Conv2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "259443f5-9c0b-451d-a2d3-9bd39ac0c848",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " time_distributed (TimeDist  (None, 18, 50, 50, 32)    4736      \n",
      " ributed)                                                        \n",
      "                                                                 \n",
      " time_distributed_1 (TimeDi  (None, 18, 48, 48, 32)    9248      \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " time_distributed_2 (TimeDi  (None, 18, 24, 24, 32)    0         \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " time_distributed_3 (TimeDi  (None, 18, 24, 24, 64)    18496     \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " time_distributed_4 (TimeDi  (None, 18, 24, 24, 64)    36928     \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " time_distributed_5 (TimeDi  (None, 18, 12, 12, 64)    0         \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " time_distributed_6 (TimeDi  (None, 18, 12, 12, 128)   73856     \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " time_distributed_7 (TimeDi  (None, 18, 12, 12, 128)   147584    \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " time_distributed_8 (TimeDi  (None, 18, 6, 6, 128)     0         \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " time_distributed_9 (TimeDi  (None, 18, 6, 6, 256)     295168    \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " time_distributed_10 (TimeD  (None, 18, 6, 6, 256)     590080    \n",
      " istributed)                                                     \n",
      "                                                                 \n",
      " time_distributed_11 (TimeD  (None, 18, 3, 3, 256)     0         \n",
      " istributed)                                                     \n",
      "                                                                 \n",
      " time_distributed_12 (TimeD  (None, 18, 3, 3, 512)     1180160   \n",
      " istributed)                                                     \n",
      "                                                                 \n",
      " time_distributed_13 (TimeD  (None, 18, 3, 3, 512)     2359808   \n",
      " istributed)                                                     \n",
      "                                                                 \n",
      " time_distributed_14 (TimeD  (None, 18, 1, 1, 512)     0         \n",
      " istributed)                                                     \n",
      "                                                                 \n",
      " time_distributed_15 (TimeD  (None, 18, 512)           0         \n",
      " istributed)                                                     \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 18, 512)           0         \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 512)               2099200   \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 5)                 2565      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 6817829 (26.01 MB)\n",
      "Trainable params: 6817829 (26.01 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#write your model here\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Conv2D,MaxPooling2D\n",
    "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
    "from keras.layers import Dense, GRU, Flatten, TimeDistributed, Flatten, BatchNormalization, Activation, Dropout, LSTM, Bidirectional\n",
    "\n",
    "\n",
    "Input_shape_1 = (18, 100, 100, 3)\n",
    "model = Sequential()\n",
    "model.add(TimeDistributed(Conv2D(32, (7, 7), strides=(2, 2), activation='relu', padding='same'), input_shape=Input_shape_1))\n",
    "model.add(TimeDistributed(Conv2D(32, (3,3), kernel_initializer=\"he_normal\", activation='relu')))\n",
    "model.add(TimeDistributed(MaxPooling2D((2, 2), strides=(2, 2))))\n",
    " \n",
    "model.add(TimeDistributed(Conv2D(64, (3,3), padding='same', activation='relu')))\n",
    "model.add(TimeDistributed(Conv2D(64, (3,3), padding='same', activation='relu')))\n",
    "model.add(TimeDistributed(MaxPooling2D((2, 2), strides=(2, 2))))\n",
    " \n",
    "model.add(TimeDistributed(Conv2D(128, (3,3), padding='same', activation='relu')))\n",
    "model.add(TimeDistributed(Conv2D(128, (3,3), padding='same', activation='relu')))\n",
    "model.add(TimeDistributed(MaxPooling2D((2, 2), strides=(2, 2))))\n",
    " \n",
    "model.add(TimeDistributed(Conv2D(256, (3,3), padding='same', activation='relu')))\n",
    "model.add(TimeDistributed(Conv2D(256, (3,3), padding='same', activation='relu')))\n",
    "model.add(TimeDistributed(MaxPooling2D((2, 2), strides=(2, 2))))\n",
    " \n",
    "model.add(TimeDistributed(Conv2D(512, (3,3), padding='same', activation='relu')))\n",
    "model.add(TimeDistributed(Conv2D(512, (3,3), padding='same', activation='relu')))\n",
    "model.add(TimeDistributed(MaxPooling2D((2, 2), strides=(2, 2))))\n",
    " \n",
    "model.add(TimeDistributed(Flatten()))\n",
    " \n",
    "model.add(Dropout(0.5))\n",
    "model.add(LSTM(512, return_sequences=False, dropout=0.5))\n",
    "model.add(Dense(5, activation='softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7b784ee7-344c-4f91-8cbe-50da23c31442",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " time_distributed (TimeDist  (None, 18, 50, 50, 32)    4736      \n",
      " ributed)                                                        \n",
      "                                                                 \n",
      " time_distributed_1 (TimeDi  (None, 18, 48, 48, 32)    9248      \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " time_distributed_2 (TimeDi  (None, 18, 24, 24, 32)    0         \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " time_distributed_3 (TimeDi  (None, 18, 24, 24, 64)    18496     \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " time_distributed_4 (TimeDi  (None, 18, 24, 24, 64)    36928     \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " time_distributed_5 (TimeDi  (None, 18, 12, 12, 64)    0         \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " time_distributed_6 (TimeDi  (None, 18, 12, 12, 128)   73856     \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " time_distributed_7 (TimeDi  (None, 18, 12, 12, 128)   147584    \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " time_distributed_8 (TimeDi  (None, 18, 6, 6, 128)     0         \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " time_distributed_9 (TimeDi  (None, 18, 6, 6, 256)     295168    \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " time_distributed_10 (TimeD  (None, 18, 6, 6, 256)     590080    \n",
      " istributed)                                                     \n",
      "                                                                 \n",
      " time_distributed_11 (TimeD  (None, 18, 3, 3, 256)     0         \n",
      " istributed)                                                     \n",
      "                                                                 \n",
      " time_distributed_12 (TimeD  (None, 18, 3, 3, 512)     1180160   \n",
      " istributed)                                                     \n",
      "                                                                 \n",
      " time_distributed_13 (TimeD  (None, 18, 3, 3, 512)     2359808   \n",
      " istributed)                                                     \n",
      "                                                                 \n",
      " time_distributed_14 (TimeD  (None, 18, 1, 1, 512)     0         \n",
      " istributed)                                                     \n",
      "                                                                 \n",
      " time_distributed_15 (TimeD  (None, 18, 512)           0         \n",
      " istributed)                                                     \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 18, 512)           0         \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 512)               2099200   \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 5)                 2565      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 6817829 (26.01 MB)\n",
      "Trainable params: 6817829 (26.01 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#optimiser = Keras.optimizers.Adam(lr=0.001)\n",
    "model.compile(optimizer='sgd', loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "print (model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d032fbb2-d697-48a8-93c5-c29528a91a9c",
   "metadata": {},
   "source": [
    "Creating Train_generator and Val_generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "22a69660-bed8-40d0-90c5-68a071bb246b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = generator(train_path, train_doc, batch_size)\n",
    "val_generator = generator(val_path, val_doc, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c17ca697-6fd4-4451-a6fb-d7605019b5f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`epsilon` argument is deprecated and will be removed, use `min_delta` instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`epsilon` argument is deprecated and will be removed, use `min_delta` instead.\n"
     ]
    }
   ],
   "source": [
    "model_name = 'Gesture_recog' + '_' + str(curr_dt_time).replace(' ','').replace(':','_') + '/'\n",
    "    \n",
    "if not os.path.exists(model_name):\n",
    "    os.mkdir(model_name)\n",
    "        \n",
    "filepath = model_name + 'model-{epoch:05d}-{loss:.5f}-{categorical_accuracy:.5f}-{val_loss:.5f}-{val_categorical_accuracy:.5f}.h5'\n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=False, save_weights_only=False, mode='auto', period=1)\n",
    "\n",
    "LR = ReduceLROnPlateau(monitor = \"val_loss\", factor = 0.1, patience = 10,\n",
    "  verbose = 0, mode = \"auto\", epsilon = 1e-04, cooldown = 0,\n",
    "  min_lr = 0)\n",
    "callbacks_list = [checkpoint, LR]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4e74e47f-5f8c-47e1-adff-8a7e301f3f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if (num_train_sequences%batch_size) == 0:\n",
    "    steps_per_epoch = int(num_train_sequences/batch_size)\n",
    "else:\n",
    "    steps_per_epoch = (num_train_sequences//batch_size) + 1\n",
    "\n",
    "if (num_val_sequences%batch_size) == 0:\n",
    "    validation_steps = int(num_val_sequences/batch_size)\n",
    "else:\n",
    "    validation_steps = (num_val_sequences//batch_size) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "60d61d91-7ce3-4675-bd90-275572768f70",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source path =  train ; batch size = 51\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] The system cannot find the path specified: 'train/WIN_20180926_17_14_32_Pro_Left_Swipe_new'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[45], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_generator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_generator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msteps_per_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msteps_per_epoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_generator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mvalidation_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_steps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mworkers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minitial_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\engine\\training.py:2810\u001b[0m, in \u001b[0;36mModel.fit_generator\u001b[1;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[0;32m   2798\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Fits the model on data yielded batch-by-batch by a Python generator.\u001b[39;00m\n\u001b[0;32m   2799\u001b[0m \n\u001b[0;32m   2800\u001b[0m \u001b[38;5;124;03mDEPRECATED:\u001b[39;00m\n\u001b[0;32m   2801\u001b[0m \u001b[38;5;124;03m  `Model.fit` now supports generators, so there is no longer any need to\u001b[39;00m\n\u001b[0;32m   2802\u001b[0m \u001b[38;5;124;03m  use this endpoint.\u001b[39;00m\n\u001b[0;32m   2803\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   2804\u001b[0m warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m   2805\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`Model.fit_generator` is deprecated and \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2806\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwill be removed in a future version. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2807\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease use `Model.fit`, which supports generators.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   2808\u001b[0m     stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[0;32m   2809\u001b[0m )\n\u001b[1;32m-> 2810\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2811\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgenerator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2812\u001b[0m \u001b[43m    \u001b[49m\u001b[43msteps_per_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msteps_per_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2813\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2814\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2815\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2816\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2817\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2818\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_freq\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_freq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2819\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclass_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2820\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_queue_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_queue_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2821\u001b[0m \u001b[43m    \u001b[49m\u001b[43mworkers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mworkers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2822\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_multiprocessing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_multiprocessing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2823\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshuffle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2824\u001b[0m \u001b[43m    \u001b[49m\u001b[43minitial_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minitial_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2825\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "Cell \u001b[1;32mIn[16], line 11\u001b[0m, in \u001b[0;36mgenerator\u001b[1;34m(source_path, folder_list, batch_size)\u001b[0m\n\u001b[0;32m      9\u001b[0m batch_labels \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros((batch_size,\u001b[38;5;241m5\u001b[39m)) \u001b[38;5;66;03m# batch_labels is the one hot representation of the output\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m folder \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(batch_size): \u001b[38;5;66;03m# iterate over the batch_size\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m     imgs \u001b[38;5;241m=\u001b[39m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource_path\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m[\u001b[49m\u001b[43mfolder\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m;\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# read all the images in the folder\u001b[39;00m\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m idx,item \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(img_idx): \u001b[38;5;66;03m#  Iterate iver the frames/images of a folder to read them in\u001b[39;00m\n\u001b[0;32m     13\u001b[0m         image \u001b[38;5;241m=\u001b[39m imageio\u001b[38;5;241m.\u001b[39mimread(source_path\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m+\u001b[39m t[folder \u001b[38;5;241m+\u001b[39m (batch\u001b[38;5;241m*\u001b[39mbatch_size)]\u001b[38;5;241m.\u001b[39mstrip()\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m;\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m+\u001b[39mimgs[item])\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mfloat32)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] The system cannot find the path specified: 'train/WIN_20180926_17_14_32_Pro_Left_Swipe_new'"
     ]
    }
   ],
   "source": [
    "model.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n",
    "                    callbacks=callbacks_list, validation_data=val_generator, \n",
    "                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "25cc947e-d2d5-494b-a5eb-34f79524b3e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(history):\n",
    "    acc = history.history['categorical_accuracy']\n",
    "    val_acc = history.history['val_categorical_accuracy']\n",
    "    loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "    epochs_range = range(num_epochs)\n",
    "    plt.figure(figsize=(15, 7))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs_range, acc, label='Training Accuracy')\n",
    "    plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.title('Training and Validation Accuracy')\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs_range, loss, label='Training Loss')\n",
    "    plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b40cffb-eab4-41b6-8d29-75033b8e70bb",
   "metadata": {},
   "source": [
    "Creating callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "1f83c506-209c-4304-bfe1-200976367207",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us save each model epochs in a directory by making check points and also save the model metrics (training accuracy, traing loss ,validation loss, validation accuracy)\n",
    "def callback_directory(curr_dt_time):\n",
    "  model_name = 'model_init' + '_' + str(curr_dt_time).replace(' ','').replace(':','_') + '/'\n",
    "    \n",
    "  if not os.path.exists(model_name):\n",
    "    os.mkdir(model_name)\n",
    "        \n",
    "  filepath = model_name + 'model-{epoch:05d}-{loss:.5f}-{categorical_accuracy:.5f}-{val_loss:.5f}-{val_categorical_accuracy:.5f}.h5'\n",
    "\n",
    "  checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=False, save_weights_only=False, mode='auto', save_freq='epoch')\n",
    "\n",
    "  LR = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2, verbose=1, mode='min', min_delta=0.0001, cooldown=0, min_lr=0.00001)\n",
    "  callbacks_list = [checkpoint, LR]\n",
    "  return callbacks_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "c9b55ccc-4331-4c90-8e05-bb5dfd55aea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the image by dividing the pixel values by 255 which resuilts the values range from 0 to 1\n",
    "def normalise1(image):\n",
    "  return image/255.\n",
    "\n",
    "def generator(source_path, folder_list, batch_size,y,z,normalise):\n",
    "    print( 'Source path = ', source_path, '; batch size =', batch_size,';Image resolution = ({},{})'.format(y,z))\n",
    "    img_idx = [0,1,2,4,6,8,10,12,14,16,18,20,22,24,26,27,28,29] #create a list of image numbers you want to use for a particular video\n",
    "    # x is the number of images you use for each video, (y,z) is the final size of the input images and 3 is the number of channels RGB\n",
    "    x=len(img_idx)\n",
    "    while True:\n",
    "        t = np.random.permutation(folder_list)\n",
    "        num_batches =  int(len(t)/batch_size)  # calculate the number of batches\n",
    "        for batch in range(num_batches): # we iterate over the number of batches\n",
    "            batch_data = np.zeros((batch_size,x,y,z,3)) # x is the number of images you use for each video, (y,z) is the final size of the input images and 3 is the number of channels RGB\n",
    "            batch_labels = np.zeros((batch_size,5)) # batch_labels is the one hot representation of the output\n",
    "            for folder in range(batch_size): # iterate over the batch_size\n",
    "                imgs = os.listdir(source_path+'/'+ t[folder + (batch*batch_size)].split(';')[0]) # read all the images in the folder\n",
    "                for idx,item in enumerate(img_idx): #  Iterate iver the frames/images of a folder to read them in\n",
    "                    image = Image.open(source_path+'/'+ t[folder + (batch*batch_size)].strip().split(';')[0]+'/'+imgs[item])\n",
    "                  \n",
    "                    #crop the images and resize them. Note that the images are of 2 different shape \n",
    "                    #and the conv3D will throw error if the inputs in a batch have different shapes\n",
    "\n",
    "                    if(image.size[0] == 160): # crop the image of shape 120 X 160 to make it  120 X120\n",
    "                      image = image.crop((20,0,140,120)) # left =20, top =0, right = 140, bottom = 120\n",
    "                    \n",
    "                    image = image.resize(size = (y,z)) #imageio.imresize(image,(y,z)).astype(np.float32)\n",
    "                    image = np.asarray(image).astype(np.float32)\n",
    "\n",
    "                    batch_data[folder,idx,:,:,0] = normalise(image[:,:,0])  #normalise and feed in the image\n",
    "                    batch_data[folder,idx,:,:,1] = normalise(image[:,:,1]) #normalise and feed in the image\n",
    "                    batch_data[folder,idx,:,:,2] = normalise(image[:,:,2]) #normalise and feed in the image\n",
    "                    \n",
    "                batch_labels[folder, int(t[folder + (batch*batch_size)].strip().split(';')[2])] = 1\n",
    "            yield batch_data, batch_labels #you yield the batch_data and the batch_labels, remember what does yield do\n",
    "\n",
    "        \n",
    "        # write the code for the remaining data points which are left after full batches\n",
    "\n",
    "        if (len(t) % batch_size) != 0:\n",
    "            batch_data = np.zeros((len(t)%batch_size,x,y,z,3))\n",
    "            batch_labels = np.zeros((len(t)%batch_size,5))\n",
    "            for folder in range(len(t)%batch_size):\n",
    "                imgs = os.listdir(source_path+'/'+ t[folder + (num_batches*batch_size)].split(';')[0])\n",
    "                for idx,item in enumerate(img_idx):\n",
    "                    image = Image.open(source_path+'/'+ t[folder + (num_batches*batch_size)].strip().split(';')[0]+'/'+imgs[item])\n",
    "                    if image.size[0] == 160:\n",
    "                        image = image.crop((20,0,140,120)) # left =20, top =0, right = 140, bottom = 120\n",
    "                    \n",
    "                    image = image.resize(size = (y,z)) \n",
    "                    image = np.asarray(image).astype(np.float32)\n",
    "\n",
    "                    \n",
    "                    batch_data[folder,idx,:,:,0] = normalise(image[:,:,0])  #normalise and feed in the image\n",
    "                    batch_data[folder,idx,:,:,1] = normalise(image[:,:,1]) #normalise and feed in the image\n",
    "                    batch_data[folder,idx,:,:,2] = normalise(image[:,:,2]) #normalise and feed in the image\n",
    "\n",
    "                batch_labels[folder, int(t[folder + (num_batches*batch_size)].strip().split(';')[2])] = 1\n",
    "\n",
    "            yield batch_data, batch_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "36b8c04a-e1e0-4197-80c2-7cc2787789f1",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2025665320.py, line 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[104], line 4\u001b[1;36m\u001b[0m\n\u001b[1;33m    self.train_doc = np.random.permutation(open(C:\\Users\\nisch\\Desktop\\Project_data + 'train.csv').readlines())\u001b[0m\n\u001b[1;37m                                                 ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "class ModelBuilder(metaclass= abc.ABCMeta):\n",
    "    # initialisng the path where project data resides\n",
    "    def initialize_path(self, Project_data):\n",
    "        self.train_doc = np.random.permutation(open(C:\\Users\\nisch\\Desktop\\Project_data + 'train.csv').readlines())\n",
    "        self.val_doc = np.random.permutation(open(C:\\Users\\nisch\\Desktop\\Project_data + 'val.csv').readlines())\n",
    "        self.train_path = \"C:\\Users\\nisch\\Desktop\\Project_data\\train.csv\" + '/' + 'train'\n",
    "        self.val_path =  \"C:\\Users\\nisch\\Desktop\\Project_data\\val.csv\" + '/' + 'val'\n",
    "        self.num_train_sequences = len(self.train_doc)\n",
    "        self.num_val_sequences = len(self.val_doc)\n",
    "    # initialising the image properties    \n",
    "    def initialize_image_properties(self,image_height=100,image_width=100):\n",
    "        self.image_height=image_height\n",
    "        self.image_width=image_width\n",
    "        self.channels=3\n",
    "        self.num_classes=5\n",
    "        self.total_frames=30\n",
    "    # initialising the batch size, frames to sample and the no. of epochs\n",
    "    def initialize_hyperparams(self,frames_to_sample=30,batch_size=20,num_epochs=20):\n",
    "        self.frames_to_sample=frames_to_sample\n",
    "        self.batch_size=batch_size\n",
    "        self.num_epochs=num_epochs\n",
    "        \n",
    "    # MOST IMPORTANT PART HERE - The generator function        \n",
    "    def generator(self,source_path, folder_list, augment=False):\n",
    "        img_idx = np.round(np.linspace(0,self.total_frames-1,self.frames_to_sample)).astype(int)\n",
    "        batch_size=self.batch_size\n",
    "        while True:\n",
    "            t = np.random.permutation(folder_list)\n",
    "            num_batches = len(t)//batch_size\n",
    "        \n",
    "            for batch in range(num_batches): \n",
    "                batch_data, batch_labels= self.one_batch_data(source_path,t,batch,batch_size,img_idx,augment)\n",
    "                yield batch_data, batch_labels \n",
    "\n",
    "            remaining_seq=len(t)%batch_size\n",
    "        \n",
    "            if (remaining_seq != 0):\n",
    "                batch_data, batch_labels= self.one_batch_data(source_path,t,num_batches,batch_size,img_idx,augment,remaining_seq)\n",
    "                yield batch_data, batch_labels \n",
    "    \n",
    "    \n",
    "    def one_batch_data(self,source_path,t,batch,batch_size,img_idx,augment,remaining_seq=0):\n",
    "    \n",
    "        seq_len = remaining_seq if remaining_seq else batch_size\n",
    "    \n",
    "        batch_data = np.zeros((seq_len,len(img_idx),self.image_height,self.image_width,self.channels)) \n",
    "        batch_labels = np.zeros((seq_len,self.num_classes)) \n",
    "    \n",
    "        if (augment): batch_data_aug = np.zeros((seq_len,len(img_idx),self.image_height,self.image_width,self.channels))\n",
    "\n",
    "        \n",
    "        for folder in range(seq_len): \n",
    "            imgs = os.listdir(source_path+'/'+ t[folder + (batch*batch_size)].split(';')[0]) \n",
    "            for idx,item in enumerate(img_idx):\n",
    "                #performing image reading and resizing\n",
    "                image = imread(source_path+'/'+ t[folder + (batch*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n",
    "                image_resized=resize(image,(self.image_height,self.image_width,3))\n",
    "            \n",
    "                #normalizing the images\n",
    "                batch_data[folder,idx,:,:,0] = (image_resized[:,:,0])/255\n",
    "                batch_data[folder,idx,:,:,1] = (image_resized[:,:,1])/255\n",
    "                batch_data[folder,idx,:,:,2] = (image_resized[:,:,2])/255\n",
    "            \n",
    "                if (augment):\n",
    "                    shifted = cv2.warpAffine(image, \n",
    "                                             np.float32([[1, 0, np.random.randint(-30,30)],[0, 1, np.random.randint(-30,30)]]), \n",
    "                                            (image.shape[1], image.shape[0]))\n",
    "                    \n",
    "                    gray = cv2.cvtColor(shifted,cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "                    x0, y0 = np.argwhere(gray > 0).min(axis=0)\n",
    "                    x1, y1 = np.argwhere(gray > 0).max(axis=0) \n",
    "                    # cropping the images to have the targeted gestures and remove the noise from the images.\n",
    "                    cropped=shifted[x0:x1,y0:y1,:]\n",
    "                    \n",
    "                    image_resized=resize(cropped,(self.image_height,self.image_width,3))\n",
    "                    \n",
    "                    #shifted = cv2.warpAffine(image_resized, \n",
    "                    #                        np.float32([[1, 0, np.random.randint(-3,3)],[0, 1, np.random.randint(-3,3)]]), \n",
    "                    #                        (image_resized.shape[1], image_resized.shape[0]))\n",
    "            \n",
    "                    batch_data_aug[folder,idx,:,:,0] = (image_resized[:,:,0])/255\n",
    "                    batch_data_aug[folder,idx,:,:,1] = (image_resized[:,:,1])/255\n",
    "                    batch_data_aug[folder,idx,:,:,2] = (image_resized[:,:,2])/255\n",
    "                \n",
    "            \n",
    "            batch_labels[folder, int(t[folder + (batch*batch_size)].strip().split(';')[2])] = 1\n",
    "            \n",
    "    \n",
    "        if (augment):\n",
    "            batch_data=np.concatenate([batch_data,batch_data_aug])\n",
    "            batch_labels=np.concatenate([batch_labels,batch_labels])\n",
    "\n",
    "        \n",
    "        return(batch_data,batch_labels)\n",
    "    \n",
    "    \n",
    "    def train_model(self, model, augment_data=False):\n",
    "        train_generator = self.generator(self.train_path, self.train_doc,augment=augment_data)\n",
    "        val_generator = self.generator(self.val_path, self.val_doc)\n",
    "\n",
    "        model_name = 'model_init' + '_' + str(datetime.datetime.now()).replace(' ','').replace(':','_') + '/'\n",
    "    \n",
    "        if not os.path.exists(model_name):\n",
    "            os.mkdir(model_name)\n",
    "        \n",
    "        filepath = model_name + 'model-{epoch:05d}-{loss:.5f}-{categorical_accuracy:.5f}-{val_loss:.5f}-{val_categorical_accuracy:.5f}.h5'\n",
    "\n",
    "        checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, save_weights_only=False, mode='auto', period=1)\n",
    "        LR = ReduceLROnPlateau(monitor='val_loss', factor=0.2, verbose=1, patience=4)\n",
    "        \n",
    "        earlystop = EarlyStopping( monitor=\"val_loss\", min_delta=0,patience=10,verbose=1)\n",
    "        callbacks_list = [checkpoint, LR, earlystop]\n",
    "\n",
    "        if (self.num_train_sequences%self.batch_size) == 0:\n",
    "            steps_per_epoch = int(self.num_train_sequences/self.batch_size)\n",
    "        else:\n",
    "            steps_per_epoch = (self.num_train_sequences//self.batch_size) + 1\n",
    "\n",
    "        if (self.num_val_sequences%self.batch_size) == 0:\n",
    "            validation_steps = int(self.num_val_sequences/self.batch_size)\n",
    "        else:\n",
    "            validation_steps = (self.num_val_sequences//self.batch_size) + 1\n",
    "    \n",
    "        history=model.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=self.num_epochs, verbose=1, \n",
    "                            callbacks=callbacks_list, validation_data=val_generator, \n",
    "                            validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)\n",
    "        return history\n",
    "\n",
    "        \n",
    "    @abc.abstractmethod\n",
    "    def define_model(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "1865099e-0d3f-4c52-9d9b-b260a20fbea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelConv3D1(ModelBuilder):\n",
    "    \n",
    "    def define_model(self):\n",
    "\n",
    "        model = Sequential()\n",
    "        model.add(Conv3D(16, (3, 3, 3), padding='same',\n",
    "                 input_shape=(self.frames_to_sample,self.image_height,self.image_width,self.channels)))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "\n",
    "        model.add(Conv3D(32, (2, 2, 2), padding='same'))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "\n",
    "        model.add(Conv3D(64, (2, 2, 2), padding='same'))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "\n",
    "        model.add(Conv3D(128, (2, 2, 2), padding='same'))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(128,activation='relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Dropout(0.5))\n",
    "\n",
    "        model.add(Dense(64,activation='relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Dropout(0.25))\n",
    "\n",
    "\n",
    "        model.add(Dense(self.num_classes,activation='softmax'))\n",
    "\n",
    "        optimiser = keras.optimizers.Adam()\n",
    "        #optimiser = 'sgd'\n",
    "        model.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "f1156e0b-d043-46c9-a9e7-7b4c5296bacb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelConv3D1(ModelBuilder):\n",
    "    \n",
    "    def define_model(self):\n",
    "\n",
    "        model = Sequential()\n",
    "        model.add(Conv3D(16, (3, 3, 3), padding='same',\n",
    "                 input_shape=(self.frames_to_sample,self.image_height,self.image_width,self.channels)))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "\n",
    "        model.add(Conv3D(32, (2, 2, 2), padding='same'))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "\n",
    "        model.add(Conv3D(64, (2, 2, 2), padding='same'))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "\n",
    "        model.add(Conv3D(128, (2, 2, 2), padding='same'))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(128,activation='relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Dropout(0.5))\n",
    "\n",
    "        model.add(Dense(64,activation='relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Dropout(0.25))\n",
    "\n",
    "\n",
    "        model.add(Dense(self.num_classes,activation='softmax'))\n",
    "\n",
    "        optimiser = keras.optimizers.Adam()\n",
    "        #optimiser = 'sgd'\n",
    "        model.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "ac760411-a63e-4e35-a4d5-265e626cbd02",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getBatchData(source_path, t, batch, batch_size, img_tensor,augment=False):\n",
    "    [x,y,z] = [len(img_tensor[0]),img_tensor[1], img_tensor[2]]\n",
    "    img_idx = img_tensor[0]\n",
    "    batch_data = np.zeros((batch_size,x,y,z,3)) # x is the number of images you use for each video, (y,z) is the final size of the input images and 3 is the number of channels RGB\n",
    "    batch_labels = np.zeros((batch_size,5)) # batch_labels is the one hot representation of the output\n",
    "    \n",
    "    if (augment): batch_data_aug = np.zeros((batch_size,x,y,z,3))\n",
    "    \n",
    "    for folder in range(batch_size): # iterate over the batch_size\n",
    "        imgs = os.listdir(source_path+'/'+ t[folder + (batch*batch_size)].split(';')[0]) # read all the images in the folder\n",
    "        for idx,item in enumerate(img_idx): #  Iterate iver the frames/images of a folder to read them in\n",
    "            image = imread(source_path+'/'+ t[folder + (batch*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n",
    "\n",
    "            #crop the images and resize them. Note that the images are of 2 different shape \n",
    "            #and the conv3D will throw error if the inputs in a batch have different shapes\n",
    "\n",
    "            # separate preprocessImage function is defined for cropping, resizing and normalizing images\n",
    "            batch_data[folder,idx,:,:,0] = preprocessImage(image[:, :, 0], y, z)\n",
    "            batch_data[folder,idx,:,:,1] = preprocessImage(image[:, :, 1], y, z)\n",
    "            batch_data[folder,idx,:,:,2] = preprocessImage(image[:, :, 2], y, z)\n",
    "            if (augment):\n",
    "                shifted = cv2.warpAffine(image, \n",
    "                                     np.float32([[1, 0, np.random.randint(-30,30)],[0, 1, np.random.randint(-30,30)]]),                                         (image.shape[1], image.shape[0]))\n",
    "                    \n",
    "                gray = cv2.cvtColor(shifted,cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "                x0, y0 = np.argwhere(gray > 0).min(axis=0)\n",
    "                x1, y1 = np.argwhere(gray > 0).max(axis=0) \n",
    "                # cropping the images to have the targeted gestures and remove the noise from the images.\n",
    "                cropped=shifted[x0:x1,y0:y1,:]\n",
    "                image_resized=imresize(cropped,(y,z,3))\n",
    "                    \n",
    "                #shifted = cv2.warpAffine(image_resized, \n",
    "                #np.float32([[1, 0, np.random.randint(-3,3)],[0, 1, np.random.randint(-3,3)]]), \n",
    "                #(image_resized.shape[1], image_resized.shape[0]))\n",
    "            \n",
    "                batch_data_aug[folder,idx,:,:,0] = (image_resized[:,:,0])/255\n",
    "                batch_data_aug[folder,idx,:,:,1] = (image_resized[:,:,1])/255\n",
    "                batch_data_aug[folder,idx,:,:,2] = (image_resized[:,:,2])/255\n",
    "\n",
    "        batch_labels[folder, int(t[folder + (batch*batch_size)].strip().split(';')[2])] = 1\n",
    "        \n",
    "        if (augment):\n",
    "            batch_data=np.concatenate([batch_data,batch_data_aug])\n",
    "            batch_labels=np.concatenate([batch_labels,batch_labels])\n",
    "    return batch_data, batch_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "c1d29441-967a-4d17-9d15-79c1137c6663",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator(source_path, folder_list, batch_size, img_tensor):\n",
    "    print( 'Source path = ', source_path, '; batch size =', batch_size)\n",
    "    while True:\n",
    "        t = np.random.permutation(folder_list)\n",
    "        num_batches = int(len(folder_list)/batch_size)\n",
    "        for batch in range(num_batches): # we iterate over the number of batches\n",
    "            yield getBatchData(source_path, t, batch, batch_size, img_tensor, True)\n",
    "        \n",
    "        # write the code for the remaining data points which are left after full batches\n",
    "        # checking if any remaining batches are there or not\n",
    "        if len(folder_list)%batch_size != 0:\n",
    "            # updated the batch size and yield\n",
    "            batch_size = len(folder_list)%batch_size\n",
    "            yield getBatchData(source_path, t, batch, batch_size, img_tensor, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "afac39a9-06f5-469c-b92c-3f54b92cb698",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# training sequences = 663\n",
      "# validation sequences = 100\n",
      "# epochs = 10\n"
     ]
    }
   ],
   "source": [
    "curr_dt_time = datetime.datetime.now()\n",
    "train_path = '../input/gesture-recognition/train'\n",
    "val_path = '../input/gesture-recognition/val'\n",
    "num_train_sequences = len(train_doc)\n",
    "print('# training sequences =', num_train_sequences)\n",
    "num_val_sequences = len(val_doc)\n",
    "print('# validation sequences =', num_val_sequences)\n",
    "num_epochs = 10\n",
    "print ('# epochs =', num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "a58c914b-02d4-430b-a378-492dab936041",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# img_tensor = [array([ 0,  2,  3,  5,  6,  8,  9, 11, 12, 14, 15, 17, 18, 20, 21, 23, 24,\n",
      "       26, 27, 29]), 100, 100, 3]\n"
     ]
    }
   ],
   "source": [
    "def getImgTensor(n_frames):\n",
    "    img_idx = np.round(np.linspace(0, 29, n_frames)).astype(int)\n",
    "    return [img_idx, 100, 100, 3]\n",
    "\n",
    "# define image tensor size\n",
    "img_tensor = getImgTensor(20)\n",
    "print ('# img_tensor =', img_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "46dd8f07-aac8-4c17-8c3d-782fcce00389",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source path =  ../input/gesture-recognition/train ; batch size = 20\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] The system cannot find the path specified: '../input/gesture-recognition/train/WIN_20180925_17_50_07_Pro_Thumbs_Down_new'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[114], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# check complete batch shape\u001b[39;00m\n\u001b[0;32m      2\u001b[0m sample_generator \u001b[38;5;241m=\u001b[39m generator(train_path, train_doc, \u001b[38;5;241m20\u001b[39m, img_tensor)\n\u001b[1;32m----> 3\u001b[0m sample_batch_data, sample_batch_labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msample_generator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(sample_batch_data\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# validation batch sample\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[111], line 7\u001b[0m, in \u001b[0;36mgenerator\u001b[1;34m(source_path, folder_list, batch_size, img_tensor)\u001b[0m\n\u001b[0;32m      5\u001b[0m num_batches \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(folder_list)\u001b[38;5;241m/\u001b[39mbatch_size)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_batches): \u001b[38;5;66;03m# we iterate over the number of batches\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m \u001b[43mgetBatchData\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimg_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# write the code for the remaining data points which are left after full batches\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# checking if any remaining batches are there or not\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(folder_list)\u001b[38;5;241m%\u001b[39mbatch_size \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;66;03m# updated the batch size and yield\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[110], line 10\u001b[0m, in \u001b[0;36mgetBatchData\u001b[1;34m(source_path, t, batch, batch_size, img_tensor, augment)\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (augment): batch_data_aug \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros((batch_size,x,y,z,\u001b[38;5;241m3\u001b[39m))\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m folder \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(batch_size): \u001b[38;5;66;03m# iterate over the batch_size\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m     imgs \u001b[38;5;241m=\u001b[39m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource_path\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m[\u001b[49m\u001b[43mfolder\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m;\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# read all the images in the folder\u001b[39;00m\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m idx,item \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(img_idx): \u001b[38;5;66;03m#  Iterate iver the frames/images of a folder to read them in\u001b[39;00m\n\u001b[0;32m     12\u001b[0m         image \u001b[38;5;241m=\u001b[39m imread(source_path\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m+\u001b[39m t[folder \u001b[38;5;241m+\u001b[39m (batch\u001b[38;5;241m*\u001b[39mbatch_size)]\u001b[38;5;241m.\u001b[39mstrip()\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m;\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m+\u001b[39mimgs[item])\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mfloat32)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] The system cannot find the path specified: '../input/gesture-recognition/train/WIN_20180925_17_50_07_Pro_Thumbs_Down_new'"
     ]
    }
   ],
   "source": [
    "# check complete batch shape\n",
    "sample_generator = generator(train_path, train_doc, 20, img_tensor)\n",
    "sample_batch_data, sample_batch_labels = next(sample_generator)\n",
    "print(sample_batch_data.shape)\n",
    "\n",
    "# validation batch sample\n",
    "sample_val_generator = generator(val_path, val_doc, 20, img_tensor)\n",
    "sample_val_batch_data, sample_val_batch_labels = next(sample_val_generator)\n",
    "print(sample_val_batch_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "439c54b7-79ec-4914-84be-b7af9a4b7f48",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
